{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf454e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "\n",
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
    "from python_speech_features import mfcc, fbank, logfbank, ssc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73cda04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:/! Models/HuBERT-large were not used when initializing HubertModel: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## CHANGE this line\n",
    "#model_path='../First experiments on speech/hubert-base-960'\n",
    "model_path='D:/! Models/HuBERT-large'\n",
    "\n",
    "### Directory for saving .npy files with features \n",
    "SAVE_PATH = 'embs'\n",
    "\n",
    "\n",
    "# Wav2Vec2 with default parameters \n",
    "extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=False, return_attention_mask=True)\n",
    "### Loading teh model\n",
    "model = HubertModel.from_pretrained(model_path, output_attentions=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3232331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster than computing H0_s via Giotto-TDA \n",
    "def prim_tree(adj_matrix):\n",
    "    infty = torch.max(adj_matrix) + 10\n",
    "    \n",
    "    dst = torch.ones(adj_matrix.shape[0]) * infty\n",
    "    visited = torch.zeros(adj_matrix.shape[0], dtype=bool)\n",
    "    ancestor = -torch.ones(adj_matrix.shape[0], dtype=int)\n",
    "\n",
    "    v, s = 0, torch.tensor(0.0)\n",
    "    for i in range(adj_matrix.shape[0] - 1):\n",
    "        visited[v] = 1\n",
    "        ancestor[dst > adj_matrix[v]] = v\n",
    "        dst = np.minimum(dst, adj_matrix[v])\n",
    "        dst[visited] = infty\n",
    "        \n",
    "        v = np.argmin(dst)\n",
    "        \n",
    "        s += adj_matrix[v][ancestor[v]]\n",
    "    return s.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7aa4fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7442/7442 [2:18:27<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "PATH = 'D:/Datasets/CREMA-D/AudioWAV' ## PAth to dataset files\n",
    "\n",
    "embs = []\n",
    "embs_zero = []\n",
    "\n",
    "### Изменить в зависимости от структуры датасета. Приведено для CREMA-D\n",
    "labels = []\n",
    "text_to_label = {'ANG' : 0, 'HAP' : 1, 'DIS' : 2, 'FEA' : 3, 'NEU' : 4, 'SAD' : 5}\n",
    "\n",
    "for filename in tqdm(os.listdir(PATH)):\n",
    "    code = filename.split('_')[-2]\n",
    "    labels.append(text_to_label[code])\n",
    "    \n",
    "    data, samplerate = sf.read(PATH + '/' + filename)\n",
    "### ----------------------------------------------------------------------\n",
    "### Baseline - concatenated embeddings from all layers\n",
    "  \n",
    "    inputs = extractor(data, sampling_rate=samplerate, return_tensors=\"pt\").input_values\n",
    "    with torch.no_grad():\n",
    "        hidden = model(inputs)[\"hidden_states\"]\n",
    "        hidden = np.asarray([layer.detach().numpy() for layer in hidden], dtype=np.float32)\n",
    "        embeddings_mean = np.mean(hidden[:, 0, :, :], 1)\n",
    "        embeddings_zero = hidden[:, 0, 0, :]\n",
    "    \n",
    "    embeddings_mean = embeddings_mean.reshape(embeddings_mean.shape[0] * embeddings_mean.shape[1])\n",
    "    embeddings_zero = embeddings_zero.reshape(embeddings_zero.shape[0] * embeddings_zero.shape[1])\n",
    "    \n",
    "    embs.append(embeddings_mean)\n",
    "    embs_zero.append(embeddings_zero)\n",
    "\n",
    "labels = np.array(labels)\n",
    "for template in [embs, embs_zero]:\n",
    "    template = np.vstack(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1e04784",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(SAVE_PATH + '/labels', labels)\n",
    "np.save(SAVE_PATH + '/X_allembs_large', embs)\n",
    "np.save(SAVE_PATH + '/X_allembszeros_large', embs_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa038a7",
   "metadata": {},
   "source": [
    "## Our toplogical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db53498c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7442/7442 [6:41:21<00:00,  3.24s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 94>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(labels)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m [features_traceh0, features_non_attention, features_extended, features_l1, features_linf]:\n\u001b[1;32m---> 95\u001b[0m     template \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    281\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "PATH = 'D:/Datasets/CREMA-D/AudioWAV' ## PAth to dataset files\n",
    "\n",
    "N_LAYERS = 12\n",
    "N_HEADS = 12\n",
    "\n",
    "features_traceh0 = []\n",
    "features_non_attention = []\n",
    "features_extended = []\n",
    "features_l1 = []\n",
    "features_linf = []\n",
    "\n",
    "### Изменить в зависимости от структуры датасета. Приведено для CREMA-D\n",
    "labels = []\n",
    "text_to_label = {'ANG' : 0, 'HAP' : 1, 'DIS' : 2, 'FEA' : 3, 'NEU' : 4, 'SAD' : 5}\n",
    "\n",
    "for filename in tqdm(os.listdir(PATH)):\n",
    "    code = filename.split('_')[-2]\n",
    "    labels.append(text_to_label[code])\n",
    "    \n",
    "    data, samplerate = sf.read(PATH + '/' + filename)\n",
    "### ---------------------------------------------------------------------- \n",
    "    mfcc_feat = mfcc(data, samplerate)\n",
    "  \n",
    "    inputs = extractor(data, sampling_rate=samplerate, return_tensors=\"pt\").input_values\n",
    "    outp = model(inputs)\n",
    "    \n",
    "    res = outp.attentions\n",
    "    embeds = outp.hidden_states\n",
    "    \n",
    "    bar_len = np.zeros(2 * N_LAYERS * N_HEADS)\n",
    "    l1_len = np.zeros(N_LAYERS * N_HEADS)\n",
    "    linf_len = np.zeros(N_LAYERS * N_HEADS) # better not use for now\n",
    "    matrix_expanded_lens = np.zeros(3 * N_LAYERS * N_HEADS)\n",
    "   \n",
    "    n_len = res[0].shape[-1]\n",
    "    for i in range(N_LAYERS):\n",
    "        for j in range(N_HEADS):\n",
    "            attention_map = res[i][0, j, :, :].detach()            \n",
    "            symm_map = 1.0 - torch.maximum(attention_map, torch.transpose(attention_map, 0,1))\n",
    "            symm_map -= torch.diag(torch.diag(symm_map))\n",
    "            '''\n",
    "            ########## BASIC FEATURES ##############################################################\n",
    "            ### Средняя длина отрезков H0                                                          #\n",
    "            bar_len[i * N_HEADS + j] = prim_tree(symm_map) / n_len                                 #\n",
    "            ### Normalized trace of the __attention map__                                          #\n",
    "            bar_len[N_HEADS * N_LAYERS + i * N_HEADS + j] = torch.mean(torch.diag(attention_map))  #\n",
    "            '''\n",
    "            ########## EXTENDED MATRIX FEATURES ####################################################\n",
    "            ### Мера несимметричности - сумма верхнетреугольной части, делёггая на n^2             #\n",
    "            matrix_expanded_lens[i * N_HEADS + j] = torch.mean(torch.triu(attention_map, 1))       #\n",
    "            ### Average atttention to next token                                                   #\n",
    "            matrix_expanded_lens[N_HEADS * N_LAYERS + i * N_HEADS + j] = torch.sum(torch.triu(attention_map, 1) - torch.triu(attention_map, 2)) / n_len\n",
    "            ### Average attention to previous token                                                #\n",
    "            matrix_expanded_lens[2 * N_HEADS * N_LAYERS + i * N_HEADS + j] = torch.sum(torch.tril(attention_map, -1) - torch.triu(attention_map, -2)) / n_len\n",
    "            ########################################################################################\n",
    "            \n",
    "            ########## H0s over L_1-distance between rows ##########################################\n",
    "            tmp = torch.cdist(attention_map, attention_map, p=1)                                   #\n",
    "            l1_len[i * N_HEADS + j] = prim_tree(tmp) / n_len                                       #\n",
    "            ########################################################################################\n",
    "            \n",
    "            ########## H0s over L_inf-distance between rows (lowest priority)#######################\n",
    "            # Computational time is too long + this feature works very bad with our                #\n",
    "            #                    traditional                 \" H0 for attention \"                  #\n",
    "            #tmp = torch.cdist(attention_map, attention_map, p=np.inf)                             #\n",
    "            #linf_len[i * N_HEADS + j] = prim_tree(tmp) / n_len                                    #\n",
    "            ########################################################################################\n",
    "    '''        \n",
    "    #################### \"Non-attentintion\" FEATURES ###############################################\n",
    "    ### Average H0 len betetween embeddings from each layer                                        #\n",
    "    layer_h0s = np.zeros(N_LAYERS + 1)                                                             #\n",
    "    for layer in range(N_LAYERS + 1):                                                              #\n",
    "        layer_h0s[layer] = prim_tree(torch.cdist(embeds[layer], embeds[layer]))  / n_len           #\n",
    "                                                                                                   #\n",
    "    ### [approximated] RTD to very last layer and to initial embeddings                            #\n",
    "    rtd_to_last = np.zeros(N_LAYERS)                                                               #\n",
    "    rtd_to_start = np.zeros(N_LAYERS)                                                              #\n",
    "    for layer in range(N_LAYERS):                                                                  #\n",
    "            rtd_to_last[layer] = rtd_r(embeds[layer], embeds[-1])                                  #\n",
    "            rtd_to_start[layer] = rtd_r(embeds[layer + 1], embeds[0])                              #                                                                   #\n",
    "                                                                                                   #\n",
    "    ### Average len H0 intervals between MFCC                                                      #\n",
    "    mfcc_torch = torch.from_numpy(mfcc_feat)                                                       #\n",
    "    h0 = prim_tree(torch.cdist(mfcc_torch, mfcc_torch))  / mfcc_feat.shape[0]                      #\n",
    "    ################################################################################################\n",
    "    '''\n",
    "    features_traceh0.append(bar_len)\n",
    "   # features_non_attention.append(np.hstack([np.mean(mfcc_feat, 0), layer_h0s, np.array([h0]), rtd_to_last, rtd_to_start]))\n",
    "    features_extended.append(matrix_expanded_lens)\n",
    "    features_l1.append(l1_len)\n",
    "    features_linf.append(linf_len)\n",
    "\n",
    "labels = np.array(labels)\n",
    "for template in [features_traceh0, features_non_attention, features_extended, features_l1, features_linf]:\n",
    "    template = np.vstack(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f68bd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for template in [features_extended, features_l1, features_linf]:\n",
    "    template = np.vstack(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb15eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_basic_l1', features_l1)\n",
    "np.save('X_basic_extended', features_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1116ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(SAVE_PATH + '/labels', labels)\n",
    "np.save(SAVE_PATH + '/X_basic', features_traceh0)\n",
    "np.save(SAVE_PATH + '/X_nonatt', features_non_attention)\n",
    "np.save(SAVE_PATH + '/X_extended', features_extended)\n",
    "np.save(SAVE_PATH + '/X_l1', features_l1)\n",
    "np.save(SAVE_PATH + '/X_linf', features_linf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d2d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_calc(X, y, seeds):\n",
    "    print('Best C:', end = ' ')\n",
    "    candidates = [0.05, 0.1, 0.2, 0.5, 0.8, 1, 2, 5] ### Candidates for parameter C\n",
    "    q = np.zeros((seeds.shape[1], 3))\n",
    "    for i in tqdm(range(seeds.shape[1])):\n",
    "        X_train, X_2, y_train, y_2 = train_test_split(X, y, test_size=0.3, random_state=seeds[0, i])\n",
    "        X_valid, X_test, y_valid, y_test = train_test_split(X_2, y_2, test_size=0.5, random_state=seeds[1, i])\n",
    "    \n",
    "        transit = StandardScaler()\n",
    "        X_train = transit.fit_transform(X_train)\n",
    "        X_valid = transit.transform(X_valid)\n",
    "        X_test = transit.transform(X_test)\n",
    "        \n",
    "        ### Grid search for best C\n",
    "        best_quality, best_c = 0.0, 1\n",
    "        for cc in candidates:\n",
    "            cls = LogisticRegression(penalty='l1', solver='liblinear', C=cc, max_iter=5000).fit(X_train, y_train)\n",
    "            candidate_quality = cls.score(X_valid, y_valid)\n",
    "            if candidate_quality > best_quality:\n",
    "                best_quality = candidate_quality\n",
    "                best_c = cc\n",
    "        cls = LogisticRegression(penalty='l1', solver='liblinear', C=best_c, max_iter=10000).fit(X_train, y_train)\n",
    "        print(best_c, end = ' ')\n",
    "        q[i][0] = cls.score(X_train, y_train) * 100.0\n",
    "        q[i][1] = cls.score(X_valid, y_valid) * 100.0\n",
    "        q[i][2] = cls.score(X_test, y_test) * 100.0\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49388c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 1/5 [09:38<38:32, 578.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 2/5 [19:15<28:52, 577.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 3/5 [28:45<19:08, 574.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 4/5 [38:19<09:34, 574.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [47:58<00:00, 575.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean est. quality : 77.67233661593555 +- 0.739547947289647\n",
      "Best C: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 1/5 [10:34<42:16, 634.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 2/5 [20:26<30:28, 609.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 3/5 [30:21<20:05, 602.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 4/5 [40:11<09:57, 597.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [50:03<00:00, 600.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero est. quality : 72.82005371530886 +- 0.8746051739888873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "N_RERUNS = 5\n",
    "seeds = np.random.randint(1000, size=(2, N_RERUNS))\n",
    "\n",
    "y = np.load(SAVE_PATH + '/labels.npy')\n",
    "### Test 0 - the most basic features\n",
    "X = np.load(SAVE_PATH + '/X_allembs_large.npy')\n",
    "q0 = do_calc(X, y, seeds)\n",
    "print('Mean est. quality :', np.mean(q0[:, -1]), '+-', np.std(q0[:, -1]))\n",
    "\n",
    "X = np.load(SAVE_PATH + '/X_allembszeros_large.npy')\n",
    "q1 = do_calc(X, y, seeds)\n",
    "print('Zero est. quality :', np.mean(q1[:, -1]), '+-', np.std(q1[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "N_RERUNS = 5\n",
    "seends = np.random.randint(1000, size=(2, N_RERUNS))\n",
    "\n",
    "y = np.load(SAVE_PATH + '/labels.npy')\n",
    "### Test 0 - the most basic features\n",
    "X = np.load(SAVE_PATH + '/X_basic.npy')\n",
    "q0 = do_calc(X, y, seeds)\n",
    "print('Basic features est. quality :', np.mean(q0[:, -1]), '+-', np.std(q0[:, -1]))\n",
    "\n",
    "X = np.load(SAVE_PATH + '/X_nonatt.npy')\n",
    "q0 = do_calc(X, y, seeds)\n",
    "print('Non-attention features est. quality :', np.mean(q0[:, -1]), '+-', np.std(q0[:, -1]))\n",
    "\n",
    "X = np.load(SAVE_PATH + '/X_extended.npy')\n",
    "q0 = do_calc(X, y, seeds)\n",
    "print('Extended features est. quality :', np.mean(q0[:, -1]), '+-', np.std(q0[:, -1]))\n",
    "\n",
    "X = np.load(SAVE_PATH + '/X_l1.npy')\n",
    "q0 = do_calc(X, y, seeds)\n",
    "print('L1 est. quality :', np.mean(q0[:, -1]), '+-', np.std(q0[:, -1]))\n",
    "\n",
    "#X = np.load(SAVE_PATH + '/X_linf.npy') --- NO!\n",
    "#q0 = do_calc(X, y, seeds)\n",
    "#print('L_inf est. quality :', np.mean(q0[:, -1]), '+-', np.std(q0[:, -1]))\n",
    "\n",
    "### Test 1 - basic features + non-attention\n",
    "X = np.hstack([np.load(SAVE_PATH + '/X_basic.npy'),\n",
    "              np.load(SAVE_PATH + '/X_nonatt.npy')])\n",
    "q1 = do_calc(X, y, seeds)\n",
    "print('Basic+non-attention features est. quality :', np.mean(q1[:, -1]), '+-', np.std(q1[:, -1]))\n",
    "\n",
    "### Test 2 - full attention\n",
    "X = np.hstack([np.load(SAVE_PATH + '/X_basic.npy'),\n",
    "              np.load(SAVE_PATH + '/X_extended.npy')])\n",
    "q2 = do_calc(X, y, seeds)\n",
    "print('all attention features est. quality :', np.mean(q2[:, -1]), '+-', np.std(q2[:, -1]))\n",
    "\n",
    "### Test 3 - l1\n",
    "X = np.hstack([np.load(SAVE_PATH + '/X_basic.npy'),\n",
    "              np.load(SAVE_PATH + '/X_l1.npy')])\n",
    "q3 = do_calc(X, y, seeds)\n",
    "print('basic + l1-features est. quality :', np.mean(q3[:, -1]), '+-', np.std(q3[:, -1]))\n",
    "\n",
    "### Test 4 - l_inf --- NO!\n",
    "#X = np.hstack([np.load(SAVE_PATH + '/X_basic.npy'),\n",
    "#              np.load(SAVE_PATH + '/X_linf.npy')])\n",
    "#q4 = do_calc(X, y, seeds)\n",
    "#print('basic + linf-features est. quality :', np.mean(q4[:, -1]), '+-', np.std(q4[:, -1]))\n",
    "\n",
    "### Test B - best for CREMA-D\n",
    "X = np.hstack([np.load(SAVE_PATH + '/X_basic.npy'),\n",
    "              np.load(SAVE_PATH + '/X_nonatt.npy'),\n",
    "              np.load(SAVE_PATH + '/X_extended.npy'),\n",
    "              np.load(SAVE_PATH + '/X_l1.npy')])\n",
    "qB = do_calc(X, y, seeds)\n",
    "print('Basic+non-attention+additional+l1 features est. quality :', np.mean(qB[:, -1]), '+-', np.std(qB[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e807c47c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
